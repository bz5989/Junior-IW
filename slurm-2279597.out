2024-12-01 00:48:25.507663: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-12-01 00:48:25.706692: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-12-01 00:48:26.413492: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/scratch/network/yw0947/.mujoco/mujoco210/bin:/usr/lib/nvidia:/scratch/network/yw0947/.mujoco/mujoco210/bin:/usr/lib/nvidia:/scratch/network/yw0947/.mujoco/mujoco210/bin:/usr/lib/nvidia
2024-12-01 00:48:26.413605: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/scratch/network/yw0947/.mujoco/mujoco210/bin:/usr/lib/nvidia:/scratch/network/yw0947/.mujoco/mujoco210/bin:/usr/lib/nvidia:/scratch/network/yw0947/.mujoco/mujoco210/bin:/usr/lib/nvidia
2024-12-01 00:48:26.413613: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2024-12-01 00:48:28 | [sd000_s_2279597.0.1733032107_ant_relabel_skills_metra_sf] Logging to /scratch/network/yw0947/Junior-IW/exp/relabel_skills_csf_ant/sd000_s_2279597.0.1733032107_ant_relabel_skills_metra_sf
2024-12-01 00:48:28 | [sd000_s_2279597.0.1733032107_ant_relabel_skills_metra_sf] Git commit: a2195918bbf42566d7113ce82cbfa919cccb8e8a
2024-12-01 00:48:28 | [sd000_s_2279597.0.1733032107_ant_relabel_skills_metra_sf] ARGS: Namespace(run_group='relabel_skills_csf_ant', n_epochs_per_eval=500, n_epochs_per_log=500, n_epochs_per_save=10000, n_epochs_per_pt_save=1000, n_epochs_per_pkl_update=None, num_random_trajectories=48, num_video_repeats=2, eval_record_video=0, eval_plot_axis=[-50.0, 50.0, -50.0, 50.0], normalizer_type='preset', frame_stack=None, video_skip_frames=1, encoder=0, spectral_normalization=0, model_master_dim=1024, model_master_num_layers=2, model_master_nonlinearity=None, sd_const_std=1, sd_batch_norm=1, use_layer_norm=0, max_path_length=200, env='ant', use_gpu=1, sample_cpu=1, seed=0, n_parallel=1, n_thread=1, n_epochs=10001, traj_batch_size=8, trans_minibatch_size=256, trans_optimization_epochs=50, common_lr=0.0001, lr_op=None, lr_te=None, dim_option=2, discrete=0, alpha=0.01, algo='relabel_skills_metra_sf', sac_tau=0.005, sac_lr_q=None, sac_lr_a=None, sac_discount=0.99, sac_scale_reward=1.0, sac_target_coef=1.0, sac_min_buffer_size=10000, sac_max_buffer_size=1000000, use_discrete_sac=0, unit_length=1, inner=1, turn_off_dones=1, num_alt_samples=100, split_group=65536, uniform_z=0, dual_reg=1, dual_lam=30, dual_slack=0.001, dual_dist='one', dual_lr=None, fixed_lam=None, self_normalizing=0, no_diff_in_rep=0, log_sum_exp=1, sample_new_z=1, num_negative_z=256, infonce_lam=5.0, diayn_include_baseline=0, add_log_sum_exp_to_rewards=0, add_penalty_to_rewards=0, metra_mlp_rep=0, goal_range=50.0, num_zero_shot_goals=50, eval_goal_metrics=1, cp_path=None, cp_path_idx=None, cp_multi_step=1, cp_unit_length=0, downstream_reward_type='esparse', downstream_num_goal_steps=50, policy_type='gaussian', cic_temp=0.5, cic_alpha=1.0, apt_knn_k=16, apt_use_icm=0, apt_rms=1, alive_reward=None, dual_dist_scaling='geom', const_scaler=1.0, wdm=0, wdm_cpc=0, wdm_idz=1, wdm_ids=0, wdm_diff=1, aug=0, joint_train=1, relabel_to_nearby_skill=True, noise_type='random_noise', noise_factor=0.15)
2024-12-01 00:48:28 | [sd000_s_2279597.0.1733032107_ant_relabel_skills_metra_sf] Setting seed to 0
2024-12-01 00:48:33.818767: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-12-01 00:48:34.016590: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-12-01 00:48:34.608759: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /scratch/network/yw0947/anaconda3/envs/csf/lib/python3.9/site-packages/cv2/../../lib64::/scratch/network/yw0947/.mujoco/mujoco210/bin:/usr/lib/nvidia:/scratch/network/yw0947/.mujoco/mujoco210/bin:/usr/lib/nvidia:/scratch/network/yw0947/.mujoco/mujoco210/bin:/usr/lib/nvidia
2024-12-01 00:48:34.608865: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /scratch/network/yw0947/anaconda3/envs/csf/lib/python3.9/site-packages/cv2/../../lib64::/scratch/network/yw0947/.mujoco/mujoco210/bin:/usr/lib/nvidia:/scratch/network/yw0947/.mujoco/mujoco210/bin:/usr/lib/nvidia:/scratch/network/yw0947/.mujoco/mujoco210/bin:/usr/lib/nvidia
2024-12-01 00:48:34.608873: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2024-12-01 00:48:35 | [sd000_s_2279597.0.1733032107_ant_relabel_skills_metra_sf] Obtaining samples...
2024-12-01 00:48:35 | [sd000_s_2279597.0.1733032107_ant_relabel_skills_metra_sf] epoch #0 | Saving snapshot...
2024-12-01 00:48:36 | [sd000_s_2279597.0.1733032107_ant_relabel_skills_metra_sf] epoch #0 | Saved
Sampling
_get_trajectories(option_policy) 10.309107542037964s
------------------------------  ----------
EvalOp/AtSuccess1                  0
EvalOp/AtSuccess3                  0.0138
EvalOp/AverageDiscountedReturn    88.0882
EvalOp/AverageReturn             202.719
EvalOp/CompletionRate              1
EvalOp/DiffMaxMinReturn           17.1931
EvalOp/EndSuccess1                 0
EvalOp/EndSuccess3                 0.02
EvalOp/GoalDistance               37.5593
EvalOp/GoalSquaredDistance      1660.81
EvalOp/HitSuccess1                 0
EvalOp/HitSuccess3                 0.02
EvalOp/Iteration                   0
EvalOp/MaxReturn                 212.416
EvalOp/MinReturn                 195.223
EvalOp/MjAvgTrajLen              200
EvalOp/MjNumCoords              9648
EvalOp/MjNumTrajs                 48
EvalOp/MjNumUniqueCoords           4
EvalOp/NumTrajs                   48
EvalOp/StdReturn                   3.93431
TimeTotal                         31.2974
TotalEnvSteps                      0
TotalEpoch                         0
------------------------------  ----------
Traceback (most recent call last):
  File "/scratch/network/yw0947/Junior-IW/run/train.py", line 1050, in <module>
    run()
    └ <garage.experiment.experiment.ExperimentTemplate object at 0x14b5e3c6cee0>
  File "/scratch/network/yw0947/Junior-IW/garaged/src/garage/experiment/experiment.py", line 594, in __call__
    result = self.function(ctxt, **kwargs)
             │             │       └ {}
             │             └ <garage.experiment.experiment.ExperimentContext object at 0x14b6ce81cca0>
             └ <garage.experiment.experiment.ExperimentTemplate object at 0x14b5e3c6cee0>
  File "/scratch/network/yw0947/Junior-IW/run/train.py", line 1045, in run
    runner.train(n_epochs=args.n_epochs, batch_size=args.traj_batch_size)
    │                     │                         └ Namespace(run_group='relabel_skills_csf_ant', n_epochs_per_eval=500, n_epochs_per_log=500, n_epochs_per_save=10000, n_epochs_per...
    │                     └ Namespace(run_group='relabel_skills_csf_ant', n_epochs_per_eval=500, n_epochs_per_log=500, n_epochs_per_save=10000, n_epochs_per...
    └ <garagei.experiment.option_local_runner.OptionLocalRunner object at 0x14b5ce0188e0>
  File "/scratch/network/yw0947/Junior-IW/garaged/src/garage/experiment/local_runner.py", line 489, in train
    average_return = self._algo.train(self)
                     │                └ <garagei.experiment.option_local_runner.OptionLocalRunner object at 0x14b5ce0188e0>
                     └ <garagei.experiment.option_local_runner.OptionLocalRunner object at 0x14b5ce0188e0>
  File "/scratch/network/yw0947/Junior-IW/iod/iod.py", line 205, in train
    self._evaluate_policy(runner)
    │                     └ <garagei.experiment.option_local_runner.OptionLocalRunner object at 0x14b5ce0188e0>
    └ <iod.relabel_skills_metra_sf.RelabelMetraSf object at 0x14b5bdf61970>
  File "/scratch/network/yw0947/anaconda3/envs/csf/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           │     │       └ {}
           │     └ (<iod.relabel_skills_metra_sf.RelabelMetraSf object at 0x14b5bdf61970>, <garagei.experiment.option_local_runner.OptionLocalRunne...
           └ <function RelabelMetraSf._evaluate_policy at 0x14b5ce02cee0>
  File "/scratch/network/yw0947/Junior-IW/iod/relabel_skills_metra_sf.py", line 901, in _evaluate_policy
    self._log_eval_metrics(runner)
    │                      └ <garagei.experiment.option_local_runner.OptionLocalRunner object at 0x14b5ce0188e0>
    └ <iod.relabel_skills_metra_sf.RelabelMetraSf object at 0x14b5bdf61970>
  File "/scratch/network/yw0947/Junior-IW/iod/iod.py", line 334, in _log_eval_metrics
    runner.eval_log_diagnostics()
    └ <garagei.experiment.option_local_runner.OptionLocalRunner object at 0x14b5ce0188e0>
  File "/scratch/network/yw0947/Junior-IW/garagei/experiment/option_local_runner.py", line 501, in eval_log_diagnostics
    dowel_wrapper.get_logger('eval').dump_all(self.step_itr)
    │                                         └ <garagei.experiment.option_local_runner.OptionLocalRunner object at 0x14b5ce0188e0>
    └ <module 'dowel_wrapper' from '/scratch/network/yw0947/Junior-IW/dowel_wrapper.py'>
  File "/scratch/network/yw0947/Junior-IW/dowel/logger.py", line 278, in dump_all
    output.dump(step=step)
    │                └ 0
    └ <dowel.simple_outputs.TextOutput object at 0x14b5cdfc3c10>
  File "/scratch/network/yw0947/Junior-IW/dowel/simple_outputs.py", line 71, in dump
    self._log_file.flush()
    └ <dowel.simple_outputs.TextOutput object at 0x14b5cdfc3c10>
OSError: [Errno 116] Stale file handle
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/scratch/network/yw0947/anaconda3/envs/csf/lib/python3.9/site-packages/tensorboardX/event_file_writer.py", line 76, in close
    self._py_recordio_writer.close()
  File "/scratch/network/yw0947/anaconda3/envs/csf/lib/python3.9/site-packages/tensorboardX/record_writer.py", line 193, in close
    self._writer.close()
OSError: [Errno 116] Stale file handle
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/scratch/network/yw0947/anaconda3/envs/csf/lib/python3.9/site-packages/tensorboardX/event_file_writer.py", line 76, in close
    self._py_recordio_writer.close()
  File "/scratch/network/yw0947/anaconda3/envs/csf/lib/python3.9/site-packages/tensorboardX/record_writer.py", line 193, in close
    self._writer.close()
OSError: [Errno 116] Stale file handle
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/scratch/network/yw0947/anaconda3/envs/csf/lib/python3.9/site-packages/tensorboardX/event_file_writer.py", line 76, in close
    self._py_recordio_writer.close()
  File "/scratch/network/yw0947/anaconda3/envs/csf/lib/python3.9/site-packages/tensorboardX/record_writer.py", line 193, in close
    self._writer.close()
OSError: [Errno 116] Stale file handle
Exception ignored in: <function LogOutput.__del__ at 0x14b6d7e6bf70>
Traceback (most recent call last):
  File "/scratch/network/yw0947/Junior-IW/dowel/logger.py", line 176, in __del__
  File "/scratch/network/yw0947/Junior-IW/dowel/simple_outputs.py", line 67, in close
OSError: [Errno 116] Stale file handle
Exception ignored in: <function LogOutput.__del__ at 0x14b6d7e6bf70>
Traceback (most recent call last):
  File "/scratch/network/yw0947/Junior-IW/dowel/logger.py", line 176, in __del__
  File "/scratch/network/yw0947/Junior-IW/dowel/tensor_board_output.py", line 174, in close
  File "/scratch/network/yw0947/anaconda3/envs/csf/lib/python3.9/site-packages/tensorboardX/writer.py", line 1237, in close
  File "/scratch/network/yw0947/anaconda3/envs/csf/lib/python3.9/site-packages/tensorboardX/writer.py", line 192, in flush
  File "/scratch/network/yw0947/anaconda3/envs/csf/lib/python3.9/site-packages/tensorboardX/event_file_writer.py", line 148, in flush
  File "/scratch/network/yw0947/anaconda3/envs/csf/lib/python3.9/site-packages/tensorboardX/event_file_writer.py", line 69, in flush
  File "/scratch/network/yw0947/anaconda3/envs/csf/lib/python3.9/site-packages/tensorboardX/record_writer.py", line 190, in flush
ValueError: flush of closed file
Exception ignored in: <function LogOutput.__del__ at 0x14b6d7e6bf70>
Traceback (most recent call last):
  File "/scratch/network/yw0947/Junior-IW/dowel/logger.py", line 176, in __del__
  File "/scratch/network/yw0947/Junior-IW/dowel/simple_outputs.py", line 67, in close
OSError: [Errno 116] Stale file handle
Exception ignored in: <function LogOutput.__del__ at 0x14b6d7e6bf70>
Traceback (most recent call last):
  File "/scratch/network/yw0947/Junior-IW/dowel/logger.py", line 176, in __del__
  File "/scratch/network/yw0947/Junior-IW/dowel/simple_outputs.py", line 67, in close
OSError: [Errno 116] Stale file handle
Exception ignored in: <function LogOutput.__del__ at 0x14b6d7e6bf70>
Traceback (most recent call last):
  File "/scratch/network/yw0947/Junior-IW/dowel/logger.py", line 176, in __del__
  File "/scratch/network/yw0947/Junior-IW/dowel/tensor_board_output.py", line 174, in close
  File "/scratch/network/yw0947/anaconda3/envs/csf/lib/python3.9/site-packages/tensorboardX/writer.py", line 1237, in close
  File "/scratch/network/yw0947/anaconda3/envs/csf/lib/python3.9/site-packages/tensorboardX/writer.py", line 192, in flush
  File "/scratch/network/yw0947/anaconda3/envs/csf/lib/python3.9/site-packages/tensorboardX/event_file_writer.py", line 148, in flush
  File "/scratch/network/yw0947/anaconda3/envs/csf/lib/python3.9/site-packages/tensorboardX/event_file_writer.py", line 69, in flush
  File "/scratch/network/yw0947/anaconda3/envs/csf/lib/python3.9/site-packages/tensorboardX/record_writer.py", line 190, in flush
ValueError: flush of closed file
Exception ignored in: <function LogOutput.__del__ at 0x14b6d7e6bf70>
Traceback (most recent call last):
  File "/scratch/network/yw0947/Junior-IW/dowel/logger.py", line 176, in __del__
  File "/scratch/network/yw0947/Junior-IW/dowel/tensor_board_output.py", line 174, in close
  File "/scratch/network/yw0947/anaconda3/envs/csf/lib/python3.9/site-packages/tensorboardX/writer.py", line 1237, in close
  File "/scratch/network/yw0947/anaconda3/envs/csf/lib/python3.9/site-packages/tensorboardX/writer.py", line 192, in flush
  File "/scratch/network/yw0947/anaconda3/envs/csf/lib/python3.9/site-packages/tensorboardX/event_file_writer.py", line 148, in flush
  File "/scratch/network/yw0947/anaconda3/envs/csf/lib/python3.9/site-packages/tensorboardX/event_file_writer.py", line 69, in flush
  File "/scratch/network/yw0947/anaconda3/envs/csf/lib/python3.9/site-packages/tensorboardX/record_writer.py", line 190, in flush
ValueError: flush of closed file
